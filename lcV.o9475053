Warning: no access to tty (Bad file descriptor).
Thus no job control in this shell.
Infiniband hardware address can be incorrect! Please read BUGS section in ifconfig(8).
eno1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 128.239.56.64  netmask 255.255.252.0  broadcast 128.239.59.255
        inet6 fe80::c654:44ff:fe55:4d7f  prefixlen 64  scopeid 0x20<link>
        ether c4:54:44:55:4d:7f  txqueuelen 1000  (Ethernet)
        RX packets 339156498  bytes 206665925252 (192.4 GiB)
        RX errors 0  dropped 472  overruns 0  frame 0
        TX packets 888503247  bytes 1242437740923 (1.1 TiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

eno2: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        ether c4:54:44:55:4d:80  txqueuelen 1000  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

ib0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 65520
        inet 192.168.56.64  netmask 255.255.252.0  broadcast 192.168.59.255
        inet6 fe80::f652:1403:b:f3c0  prefixlen 64  scopeid 0x20<link>
        infiniband 80:00:00:29:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00  txqueuelen 256  (InfiniBand)
        RX packets 772751748  bytes 3361581406081 (3.0 TiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 376855292  bytes 368123747503 (342.8 GiB)
        TX errors 0  dropped 22450 overruns 0  carrier 0  collisions 0

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 13616594  bytes 161586469754 (150.4 GiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 13616594  bytes 161586469754 (150.4 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
======================================================
Environment variables set by the agent on PID 29584:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '1',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '58912',
 'RANK': '1',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 29583:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '0',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '58912',
 'RANK': '0',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 29585:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '2',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '58912',
 'RANK': '2',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 29592:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '9',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '58912',
 'RANK': '9',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================
======================================================
Environment variables set by the agent on PID 29593:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '10',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '58912',
 'RANK': '10',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================


======================================================
Environment variables set by the agent on PID 29591:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '8',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '58912',
 'RANK': '8',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 29589:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '6',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '58912',
 'RANK': '6',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 29588:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '5',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '58912',
 'RANK': '5',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 29587:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '4',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '58912',
 'RANK': '4',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 29594:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '11',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '58912',
 'RANK': '11',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 29590:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '7',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '58912',
 'RANK': '7',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 29586:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '3',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '58912',
 'RANK': '3',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

On PID 29584, after init process group, rank=1, world_size = 12
On PID 29588, after init process group, rank=5, world_size = 12
On PID 29594, after init process group, rank=11, world_size = 12

On PID 29593, after init process group, rank=10, world_size = 12
On PID 29592, after init process group, rank=9, world_size = 12
On PID 29589, after init process group, rank=6, world_size = 12

On PID 29586, after init process group, rank=3, world_size = 12
On PID 29590, after init process group, rank=7, world_size = 12





On PID 29583, after init process group, rank=0, world_size = 12


On PID 29585, after init process group, rank=2, world_size = 12

On PID 29591, after init process group, rank=8, world_size = 12

On PID 29587, after init process group, rank=4, world_size = 12

trainer_5
trainer_6
trainer_1
trainer_2
trainer_11
trainer_10
trainer_9
trainer_3
trainer_7
trainer_4
trainer_8
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.
[W tensorpipe_agent.cpp:681] RPC agent for trainer_9 encountered error when reading incoming request from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:681] RPC agent for trainer_8 encountered error when reading incoming request from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:886] RPC agent for trainer_6 encountered error when reading incoming response from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:681] RPC agent for trainer_6 encountered error when reading incoming request from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:681] RPC agent for trainer_7 encountered error when reading incoming request from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:886] RPC agent for trainer_1 encountered error when reading incoming response from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:681] RPC agent for trainer_1 encountered error when reading incoming request from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:886] RPC agent for trainer_2 encountered error when reading incoming response from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:681] RPC agent for trainer_2 encountered error when reading incoming request from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:886] RPC agent for trainer_3 encountered error when reading incoming response from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:681] RPC agent for trainer_11 encountered error when reading incoming request from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:681] RPC agent for trainer_3 encountered error when reading incoming request from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:886] RPC agent for trainer_4 encountered error when reading incoming response from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:886] RPC agent for trainer_5 encountered error when reading incoming response from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:681] RPC agent for trainer_4 encountered error when reading incoming request from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:681] RPC agent for trainer_5 encountered error when reading incoming request from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_6 encountered error when sending outgoing request #7 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:681] RPC agent for trainer_10 encountered error when reading incoming request from parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_6 encountered error when sending outgoing request #8 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_1 encountered error when sending outgoing request #7 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_2 encountered error when sending outgoing request #7 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_5 encountered error when sending outgoing request #7 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_2 encountered error when sending outgoing request #8 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_3 encountered error when sending outgoing request #5 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_1 encountered error when sending outgoing request #8 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_6 encountered error when sending outgoing request #9 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_5 encountered error when sending outgoing request #8 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_3 encountered error when sending outgoing request #6 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_4 encountered error when sending outgoing request #5 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_4 encountered error when sending outgoing request #6 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_2 encountered error when sending outgoing request #9 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_1 encountered error when sending outgoing request #9 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_5 encountered error when sending outgoing request #9 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_3 encountered error when sending outgoing request #7 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_6 encountered error when sending outgoing request #10 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_4 encountered error when sending outgoing request #7 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_2 encountered error when sending outgoing request #10 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_1 encountered error when sending outgoing request #10 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_5 encountered error when sending outgoing request #10 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
Traceback (most recent call last):
  File "/sciclone/home20/hmbaier/lc_v2/run2.py", line 211, in <module>
[W tensorpipe_agent.cpp:862] RPC agent for trainer_3 encountered error when sending outgoing request #8 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
    setup_rpc()
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/sciclone/home20/hmbaier/lc_v2/run2.py", line 122, in setup_rpc
    run_worker(dist.get_rank(), train_dl)
  File "/sciclone/home20/hmbaier/lc_v2/utils2.py", line 324, in run_worker
    run_training_loop(dist.get_rank(), 0, train_dl, 0)
  File "/sciclone/home20/hmbaier/lc_v2/utils2.py", line 377, in run_training_loop
    
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sciclone/home20/hmbaier/lc_v2/utils2.py", line 419, in forward
    ParameterServer.get_param_rrefs,
  File "/sciclone/home20/hmbaier/lc_v2/utils2.py", line 83, in remote_method
    return rpc.rpc_sync(rref.owner(), _call_method, args=args, kwargs=kwargs)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/rpc/api.py", line 77, in wrapper
    return func(*args, **kwargs)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/rpc/api.py", line 765, in rpc_sync
    return fut.wait()
RuntimeError: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_4 encountered error when sending outgoing request #8 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
[W tensorpipe_agent.cpp:862] RPC agent for trainer_6 encountered error when sending outgoing request #11 to parameter_server: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
Traceback (most recent call last):
  File "/sciclone/home20/hmbaier/lc_v2/run2.py", line 211, in <module>
Traceback (most recent call last):
  File "/sciclone/home20/hmbaier/lc_v2/run2.py", line 211, in <module>
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 29584 closing signal SIGTERM
    setup_rpc()
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    setup_rpc()WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 29585 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 29586 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 29587 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 29588 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 29589 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 29590 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 29591 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 29592 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 29593 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 29594 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 29583) of binary: /sciclone/home20/hmbaier/.conda/envs/dhsrl4/bin/python
Traceback (most recent call last):
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.10.0', 'console_scripts', 'torchrun')())
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/run.py", line 719, in main
    run(args)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
/sciclone/home20/hmbaier/lc_v2/run2.py FAILED
------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-01-23_16:00:29
  host      : vx01.sciclone.wm.edu
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 29583)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 29583
======================================================
