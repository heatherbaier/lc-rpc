Warning: no access to tty (Bad file descriptor).
Thus no job control in this shell.
Infiniband hardware address can be incorrect! Please read BUGS section in ifconfig(8).
eno1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 128.239.56.64  netmask 255.255.252.0  broadcast 128.239.59.255
        inet6 fe80::c654:44ff:fe55:4d7f  prefixlen 64  scopeid 0x20<link>
        ether c4:54:44:55:4d:7f  txqueuelen 1000  (Ethernet)
        RX packets 341942168  bytes 210869883651 (196.3 GiB)
        RX errors 0  dropped 472  overruns 0  frame 0
        TX packets 888755188  bytes 1242460729834 (1.1 TiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

eno2: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        ether c4:54:44:55:4d:80  txqueuelen 1000  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

ib0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 65520
        inet 192.168.56.64  netmask 255.255.252.0  broadcast 192.168.59.255
        inet6 fe80::f652:1403:b:f3c0  prefixlen 64  scopeid 0x20<link>
        infiniband 80:00:00:29:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00  txqueuelen 256  (InfiniBand)
        RX packets 772752512  bytes 3361581460432 (3.0 TiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 376855373  bytes 368123754387 (342.8 GiB)
        TX errors 0  dropped 22450 overruns 0  carrier 0  collisions 0

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 13656900  bytes 162117332146 (150.9 GiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 13656900  bytes 162117332146 (150.9 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
======================================================
Environment variables set by the agent on PID 30500:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '11',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '52514',
 'RANK': '11',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 30494:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '5',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '52514',
 'RANK': '5',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 30499:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '10',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '52514',
 'RANK': '10',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 30489:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '0',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '52514',
 'RANK': '0',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 30492:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '3',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '52514',
 'RANK': '3',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 30496:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '7',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '52514',
 'RANK': '7',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 30497:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '8',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '52514',
 'RANK': '8',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 30495:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '6',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '52514',
 'RANK': '6',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 30493:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '4',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '52514',
 'RANK': '4',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 30490:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '1',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '52514',
 'RANK': '1',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 30498:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '9',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '52514',
 'RANK': '9',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

======================================================
Environment variables set by the agent on PID 30491:
{'GROUP_RANK': '0',
 'LOCAL_RANK': '2',
 'MASTER_ADDR': 'vx01.sciclone.wm.edu',
 'MASTER_PORT': '52514',
 'RANK': '2',
 'TORCHELASTIC_MAX_RESTARTS': '0',
 'TORCHELASTIC_RESTART_COUNT': '0',
 'WORLD_SIZE': '12'}
======================================================

On PID 30492, after init process group, rank=3, world_size = 12

On PID 30489, after init process group, rank=0, world_size = 12
On PID 30497, after init process group, rank=8, world_size = 12


On PID 30493, after init process group, rank=4, world_size = 12
On PID 30494, after init process group, rank=5, world_size = 12
On PID 30490, after init process group, rank=1, world_size = 12



On PID 30491, after init process group, rank=2, world_size = 12

On PID 30500, after init process group, rank=11, world_size = 12

On PID 30496, after init process group, rank=7, world_size = 12

On PID 30499, after init process group, rank=10, world_size = 12
On PID 30498, after init process group, rank=9, world_size = 12


On PID 30495, after init process group, rank=6, world_size = 12

trainer_10
trainer_11
trainer_2
trainer_9
trainer_5
trainer_6
trainer_7
trainer_4
trainer_8
trainer_3
trainer_1
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
DONE UP TO HERE IN TRAINING LOOP!!!!
[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.
[W python_anomaly_mode.cpp:104] Warning: Error detected in NativeBatchNormBackward0. Traceback of forward call that caused the error:
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/rpc/internal.py", line 204, in _run_function
    result = python_udf.func(*python_udf.args, **python_udf.kwargs)
  File "/sciclone/home20/hmbaier/lc_v2/utils2.py", line 71, in _call_method
    return method(rref.local_value(), *args, **kwargs)
  File "/sciclone/home20/hmbaier/lc_v2/utils2.py", line 454, in forward
    out = self.model(lc_im, census, im, muni_id)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sciclone/home20/hmbaier/lc_v2/models.py", line 175, in forward
    return self.conv_net(glimpse)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sciclone/home20/hmbaier/lc_v2/models.py", line 62, in forward
    x = self.layer1(x)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torchvision/models/resnet.py", line 71, in forward
    out = self.bn1(out)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 168, in forward
    return F.batch_norm(
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/nn/functional.py", line 2282, in batch_norm
    return torch.batch_norm(
 (function _print_stack)
Traceback (most recent call last):
  File "/sciclone/home20/hmbaier/lc_v2/run2.py", line 211, in <module>
    setup_rpc()
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/sciclone/home20/hmbaier/lc_v2/run2.py", line 122, in setup_rpc
    run_worker(dist.get_rank(), train_dl)
  File "/sciclone/home20/hmbaier/lc_v2/utils2.py", line 324, in run_worker
    run_training_loop(dist.get_rank(), 0, train_dl, 0)
  File "/sciclone/home20/hmbaier/lc_v2/utils2.py", line 384, in run_training_loop
    dist_autograd.backward(cid, [loss])
RuntimeError: Error on Node 0: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64]] is at version 5; expected version 4 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
Exception raised from unpack at /opt/conda/conda-bld/pytorch_1634272107467/work/torch/csrc/autograd/saved_variable.cpp:164 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x2b229de1ad62 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x5b (0x2b229de1768b in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: torch::autograd::SavedVariable::unpack(std::shared_ptr<torch::autograd::Node>) const + 0x81d (0x2b229674ff5d in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
frame #3: torch::autograd::generated::NativeBatchNormBackward0::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x126 (0x2b2296050966 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
frame #4: <unknown function> + 0x34d28a7 (0x2b22967138a7 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
frame #5: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x145b (0x2b229670eb0b in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
frame #6: torch::distributed::autograd::DistEngine::execute_graph_task_until_ready_queue_empty(torch::autograd::NodeTask&&, bool) + 0x3e3 (0x2b2296e7c533 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
frame #7: <unknown function> + 0x3c3bde0 (0x2b2296e7cde0 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
frame #8: <unknown function> + 0xe19c38 (0x2b229405ac38 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
frame #9: c10::ThreadPool::main_loop(unsigned long) + 0x283 (0x2b229de0b9b3 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0xcb8f (0x2b2293035b8f in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch.so)
frame #11: <unknown function> + 0x7ea5 (0x2b22837f0ea5 in /lib64/libpthread.so.0)
frame #12: clone + 0x6d (0x2b2283b079fd in /lib64/libc.so.6)

[W tensorpipe_agent.cpp:681] RPC agent for parameter_server encountered error when reading incoming request from trainer_9: EOF: end of file (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30489 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30490 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30491 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30492 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30493 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30494 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30495 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30496 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30497 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30499 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 30500 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 9 (pid: 30498) of binary: /sciclone/home20/hmbaier/.conda/envs/dhsrl4/bin/python
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:no error file defined for parent, to copy child error file (/local/scr/hmbaier/TMPDIR/torchelastic_f12oeq7o/790876_5k8jsb3n/attempt_0/9/error.json)
Traceback (most recent call last):
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.10.0', 'console_scripts', 'torchrun')())
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/run.py", line 719, in main
    run(args)
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/sciclone/home20/hmbaier/lc_v2/run2.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-01-23_16:02:18
  host      : vx01.sciclone.wm.edu
  rank      : 9 (local_rank: 9)
  exitcode  : 1 (pid: 30498)
  error_file: /local/scr/hmbaier/TMPDIR/torchelastic_f12oeq7o/790876_5k8jsb3n/attempt_0/9/error.json
  traceback : Traceback (most recent call last):
    File "/sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
      return f(*args, **kwargs)
    File "/sciclone/home20/hmbaier/lc_v2/run2.py", line 122, in setup_rpc
      run_worker(dist.get_rank(), train_dl)
    File "/sciclone/home20/hmbaier/lc_v2/utils2.py", line 324, in run_worker
      run_training_loop(dist.get_rank(), 0, train_dl, 0)
    File "/sciclone/home20/hmbaier/lc_v2/utils2.py", line 384, in run_training_loop
      dist_autograd.backward(cid, [loss])
  RuntimeError: Error on Node 0: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64]] is at version 5; expected version 4 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
  Exception raised from unpack at /opt/conda/conda-bld/pytorch_1634272107467/work/torch/csrc/autograd/saved_variable.cpp:164 (most recent call first):
  frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x2b229de1ad62 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libc10.so)
  frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x5b (0x2b229de1768b in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libc10.so)
  frame #2: torch::autograd::SavedVariable::unpack(std::shared_ptr<torch::autograd::Node>) const + 0x81d (0x2b229674ff5d in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
  frame #3: torch::autograd::generated::NativeBatchNormBackward0::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x126 (0x2b2296050966 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
  frame #4: <unknown function> + 0x34d28a7 (0x2b22967138a7 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
  frame #5: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x145b (0x2b229670eb0b in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
  frame #6: torch::distributed::autograd::DistEngine::execute_graph_task_until_ready_queue_empty(torch::autograd::NodeTask&&, bool) + 0x3e3 (0x2b2296e7c533 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
  frame #7: <unknown function> + 0x3c3bde0 (0x2b2296e7cde0 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
  frame #8: <unknown function> + 0xe19c38 (0x2b229405ac38 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
  frame #9: c10::ThreadPool::main_loop(unsigned long) + 0x283 (0x2b229de0b9b3 in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libc10.so)
  frame #10: <unknown function> + 0xcb8f (0x2b2293035b8f in /sciclone/home20/hmbaier/.conda/envs/dhsrl4/lib/python3.9/site-packages/torch/lib/libtorch.so)
  frame #11: <unknown function> + 0x7ea5 (0x2b22837f0ea5 in /lib64/libpthread.so.0)
  frame #12: clone + 0x6d (0x2b2283b079fd in /lib64/libc.so.6)
  
  
============================================================
